{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, BertForMaskedLM, AdamW, BertConfig, BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make training & validation set with toeic part5 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Part5_training.txt', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "training, val = train_test_split(dataset, test_size = 0.1, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "toeic_training = []\n",
    "\n",
    "for pset in training:\n",
    "    q = pset['question'].strip().lower()\n",
    "    ans = pset['answer'].strip().lower()\n",
    "    for j in range(1, 5):\n",
    "        if pset[str(j)].strip().lower() == ans:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        sentence = re.sub('_', ' '+pset[str(j)]+' ', q).lower()\n",
    "        \n",
    "        input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        attn_mask = [1]*len(input_ids) + [0]*(max_len - len(input_ids))\n",
    "        input_ids = input_ids + [0]*(max_len-len(input_ids))\n",
    "        segment_ids = [0] * max_len\n",
    "        if label == 1:\n",
    "            for _ in range(3):\n",
    "                toeic_training.append([input_ids, segment_ids, attn_mask, label])\n",
    "        else:\n",
    "            toeic_training.append([input_ids, segment_ids, attn_mask, label])\n",
    "random.shuffle(toeic_training)\n",
    "\n",
    "\n",
    "toeic_val = []\n",
    "\n",
    "for pset in val:\n",
    "    q = pset['question'].strip().lower()\n",
    "    ans = pset['answer'].strip().lower()\n",
    "    for j in range(1, 5):\n",
    "        if pset[str(j)].strip().lower() == ans:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        sentence = re.sub('_', ' '+pset[str(j)]+' ', q).lower()\n",
    "        input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        attn_mask = [1]*len(input_ids) + [0]*(max_len - len(input_ids))\n",
    "        input_ids = input_ids + [0]*(max_len-len(input_ids))\n",
    "        segment_ids = [0] * max_len\n",
    "        toeic_val.append([input_ids, segment_ids, attn_mask, label])\n",
    "random.shuffle(toeic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27996"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toeic_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size : 16, num of batch_train : 1750, num of batch_validation : 130\n"
     ]
    }
   ],
   "source": [
    "input_ids, segment_ids, attn_mask, label = zip(*toeic_training)\n",
    "input_ids = torch.LongTensor(input_ids)\n",
    "segment_ids = torch.LongTensor(segment_ids)\n",
    "label = torch.LongTensor(label)\n",
    "attn_mask = torch.LongTensor(attn_mask)\n",
    "input_ids_loader = torch.utils.data.DataLoader(input_ids, batch_size=16)\n",
    "segment_ids_loader = torch.utils.data.DataLoader(segment_ids, batch_size=16)\n",
    "label_loader = torch.utils.data.DataLoader(label, batch_size=16)\n",
    "attn_mask_loader = torch.utils.data.DataLoader(attn_mask, batch_size=16)\n",
    "\n",
    "#validation\n",
    "input_ids_val, segment_ids_val, attn_mask_val, label_val = zip(*toeic_val)\n",
    "input_ids_val = torch.LongTensor(input_ids_val)\n",
    "segment_ids_val = torch.LongTensor(segment_ids_val)\n",
    "label_val = torch.LongTensor(label_val)\n",
    "attn_mask_val = torch.LongTensor(attn_mask_val)\n",
    "input_ids_loader_val = torch.utils.data.DataLoader(input_ids_val, batch_size=16)\n",
    "segment_ids_loader_val = torch.utils.data.DataLoader(segment_ids_val, batch_size=16)\n",
    "label_loader_val = torch.utils.data.DataLoader(label_val, batch_size=16)\n",
    "attn_mask_loader_val = torch.utils.data.DataLoader(attn_mask_val, batch_size=16)\n",
    "\n",
    "print('batch size : 16, num of batch_train : {}, num of batch_validation : {}'.format(len(input_ids_loader), len(input_ids_loader_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning BertForSequenceClassification model(bert_grammer) with toeic part5 questions(4666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_grammer = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "bert_grammer.cuda()\n",
    "\n",
    "optimizer = AdamW(bert_grammer.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "epochs = 5\n",
    "\n",
    "#number of batches * epochs = total number of training step\n",
    "total_steps = len(input_ids_loader) * epochs\n",
    "#create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch   300  of  1,750.\n",
      "  Batch   600  of  1,750.\n",
      "  Batch   900  of  1,750.\n",
      "  Batch 1,200  of  1,750.\n",
      "  Batch 1,500  of  1,750.\n",
      "\n",
      "Average training loss: 0.50\n",
      "\n",
      "Running Validaiton...\n",
      "Validation loss decreased ( inf --> 0.572919). Saving model ...\n",
      "Average validation loss: 0.57\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch   300  of  1,750.\n",
      "  Batch   600  of  1,750.\n",
      "  Batch   900  of  1,750.\n",
      "  Batch 1,200  of  1,750.\n",
      "  Batch 1,500  of  1,750.\n",
      "\n",
      "Average training loss: 0.27\n",
      "\n",
      "Running Validaiton...\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Average validation loss: 0.80\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch   300  of  1,750.\n",
      "  Batch   600  of  1,750.\n",
      "  Batch   900  of  1,750.\n",
      "  Batch 1,200  of  1,750.\n",
      "  Batch 1,500  of  1,750.\n",
      "\n",
      "Average training loss: 0.18\n",
      "\n",
      "Running Validaiton...\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Average validation loss: 0.87\n",
      "Early stopping executed\n",
      "Finetuning Bert for grammer is finished!\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "early_stopping = EarlyStopping(verbose = True)\n",
    "for epoch in range(0, epochs):\n",
    "    \n",
    "    #train\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "    print('Training...')\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    bert_grammer.train()\n",
    "    step = 0\n",
    "    for input_ids, segment_ids, label, attn_mask in zip(input_ids_loader, segment_ids_loader, label_loader, attn_mask_loader):\n",
    "        \n",
    "        if step % 300 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(input_ids_loader)))\n",
    "        step += 1\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        segment_ids = segment_ids.to('cuda')\n",
    "        label = label.to('cuda')\n",
    "        attn_mask = attn_mask.to('cuda')\n",
    "        bert_tuned.zero_grad()\n",
    "        outputs = bert_grammer(input_ids=input_ids ,token_type_ids=segment_ids, labels = label, attention_mask = attn_mask)\n",
    "        clf_loss = outputs[0]\n",
    "        train_loss += clf_loss.item()\n",
    "        clf_loss.backward()\n",
    "        \n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(bert_grammer.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        #Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \n",
    "    avg_train_loss = train_loss / len(input_ids_loader)\n",
    "    loss_train.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    print(\"\")\n",
    "    print('Running Validaiton...')\n",
    "    bert_grammer.eval()\n",
    "    \n",
    "    for input_ids_val, segment_ids_val, label_val, attn_mask_val in zip(input_ids_loader_val, segment_ids_loader_val, label_loader_val, attn_mask_loader_val):\n",
    "        input_ids_val = input_ids_val.to('cuda')\n",
    "        segment_ids_val = segment_ids_val.to('cuda')\n",
    "        label_val = label_val.to('cuda')\n",
    "        attn_mask_val = attn_mask_val.to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bert_grammer(input_ids=input_ids_val ,token_type_ids=segment_ids_val, labels = label_val, attention_mask = attn_mask_val)\n",
    "        clf_loss_val = outputs[0]\n",
    "        val_loss += clf_loss_val.item()\n",
    "    avg_val_loss = val_loss / len(input_ids_loader_val)\n",
    "    loss_val.append(avg_val_loss)\n",
    "    \n",
    "    early_stopping(avg_val_loss, bert_grammer)\n",
    "    print(\"Average validation loss: {0:.2f}\".format(avg_val_loss))\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping executed\")\n",
    "        break\n",
    "        \n",
    "    bert_tuned.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    \n",
    "print(\"Finetuning Bert for grammer is finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models with toeic part5 915 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get score of Finetuned SequenceClassification model(bert_grammer)\n",
    "def get_logit(model, input_ids, segment_ids):\n",
    "    input_ids_tensor = torch.tensor(input_ids).unsqueeze(0).to('cuda')\n",
    "    segment_ids_tensor = torch.tensor(segment_ids).to('cuda')\n",
    "    outputs = model(input_ids = input_ids_tensor, token_type_ids = segment_ids_tensor)\n",
    "    logit = outputs[0][0][1]\n",
    "\n",
    "    return logit.item()\n",
    "\n",
    "#function to get score or pretrained BertForMaskedLM\n",
    "def get_score(model, tokenizer, question_tensors, segment_tensors, masked_index, candidate):\n",
    "    \n",
    "    question_tensors = torch.tensor(question_tensors).unsqueeze(0).to('cuda')\n",
    "    segment_tensors = torch.tensor(segment_tensors).to('cuda')\n",
    "\n",
    "    candidate_tokens = tokenizer.tokenize(candidate) # warranty -> ['warrant', '##y']\n",
    "    candidate_ids = tokenizer.convert_tokens_to_ids(candidate_tokens)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_ids = question_tensors, token_type_ids = segment_tensors)\n",
    "        predictions_candidates = predictions[0][0][masked_index][candidate_ids].mean()\n",
    "\n",
    "    return predictions_candidates.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "915 Toeic part5 questions are loaded! Our model will solve 915 questions like below.\n",
      "\n",
      "=================================================Quesiton Example==================================================\n",
      "question : The marketing seminar is being [ ? ] from August 8th through the 11th at Rupp Convention Center.\n",
      "answer : held\n",
      "1 : held\n",
      "2 : holds\n",
      "3 : holding\n",
      "4 : hold\n",
      "\n",
      "\n",
      "Testing 100 in 915\n",
      "Testing 200 in 915\n",
      "Testing 300 in 915\n",
      "Testing 400 in 915\n",
      "Testing 500 in 915\n",
      "Testing 600 in 915\n",
      "Testing 700 in 915\n",
      "Testing 800 in 915\n",
      "Testing 900 in 915\n",
      "==========================================================================Precision==========================================================================\n",
      "Pretrained LMmodel : 0.8382513661202186, Finetuned Bert_grammer : 0.8666666666666667, Mixed(LM + Bert_grammer)  Model : 0.8874316939890711\n"
     ]
    }
   ],
   "source": [
    "#Testing BertForMaskedLM(only pretrained) + BertForSequenceClassification(Grammer finetuned)\n",
    "bert_lm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bert_lm.cuda()\n",
    "bert_lm.eval()\n",
    "bert_grammer.eval()\n",
    "cnt_bert_mixed = 0\n",
    "cnt_bert_base = 0\n",
    "cnt_bert_grammer = 0\n",
    "\n",
    "with open('Part5_test.txt', 'rb') as f:\n",
    "    testset = pickle.load(f)\n",
    "\n",
    "print(f'\\n{len(testset)} Toeic part5 questions are loaded! Our model will solve {len(testset)} questions like below.\\n')\n",
    "print('=================================================Quesiton Example==================================================')\n",
    "for k, v in testset[random.randrange(1, len(testset))].items():\n",
    "    if k == 'question':\n",
    "        v = re.sub('_', '[ ? ]', v)\n",
    "    print(f'{k} : {v}')\n",
    "print('\\n')\n",
    "for i, pset in enumerate(testset):\n",
    "    \n",
    "    grammer_score = []\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(\"Testing {} in {}\".format(i+1, len(testset)))\n",
    "    q = pset['question'].lower()\n",
    "    ans = pset['answer'].lower() \n",
    "    sentence_lm = re.sub('_', ' [MASK] ', q)\n",
    "    input_ids_lm = tokenizer.encode(sentence_lm, add_special_tokens=True)\n",
    "    masked_index_lm = input_ids_lm.index(103)\n",
    "    segment_ids_lm = [0] * len(input_ids_lm)\n",
    "    lm_score = [get_score(bert_lm, tokenizer, input_ids_lm, segment_ids_lm, masked_index_lm, pset[str(j)])  for j in range(1, 5)]\n",
    "    \n",
    "    for k in range(1, 5):\n",
    "        sentence_grammer = re.sub('_', ' '+pset[str(k)]+' ', q).lower()\n",
    "        input_ids_grammer = tokenizer.encode(sentence_grammer, add_special_tokens=True)\n",
    "        segment_ids_grammer = [0] * len(input_ids_grammer)\n",
    "        grammer_score.append(get_logit(bert_tuned, input_ids_grammer, segment_ids_grammer))\n",
    "    \n",
    "    softmax = torch.nn.Softmax(dim=0)\n",
    "    total_score = softmax(torch.tensor(lm_score)) + softmax(torch.tensor(grammer_score))\n",
    "    #print(lm_score, grammer_score, total_score)\n",
    "    pred_tunedModel = np.argmax(grammer_score)+1\n",
    "    pred_baseModel = torch.argmax(softmax(torch.tensor(lm_score))).item()+1\n",
    "    \n",
    "    if any([len(pset[str(j)].strip().split()) >= 2 for j in range(1, 5)]):\n",
    "        pred_mixedModel = np.argmax(grammer_score)+1\n",
    "    else:\n",
    "        pred_mixedModel = torch.argmax(total_score).item()+1\n",
    "        \n",
    "    if pset[str(pred_tunedModel)].lower() == ans:\n",
    "        cnt_bert_grammer += 1\n",
    "    if pset[str(pred_mixedModel)].lower() == ans:\n",
    "        cnt_bert_mixed += 1\n",
    "    if pset[str(pred_baseModel)].lower() == ans:\n",
    "        cnt_bert_base += 1\n",
    "        \n",
    "print('==========================================================================Precision==========================================================================')\n",
    "print('Pretrained LMmodel : {}, Finetuned Bert_grammer : {}, Mixed(LM + Bert_grammer)  Model : {}'.format(cnt_bert_base/len(testset), cnt_bert_grammer/len(testset), cnt_bert_mixed/len(testset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
